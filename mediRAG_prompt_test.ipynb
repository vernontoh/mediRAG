{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mediRAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from prompts import *\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)  # have to change depending on which device u wana use\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"models\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bigbio/pubmed_qa\", cache_dir=\"data\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content_column = \"CONTEXTS\"\n",
    "\n",
    "def preprocess(dataset):\n",
    "    for split in dataset.keys():\n",
    "        for contexts in dataset[split][page_content_column]:\n",
    "            for sentence in contexts:\n",
    "                yield Document(page_content=sentence)\n",
    "\n",
    "data = list(preprocess(dataset))  # 655055\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device':'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model,   \n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs, \n",
    "    cache_folder=\"models\"\n",
    ")\n",
    "\n",
    "if os.path.exists(\"faiss_index_pubmed\"):\n",
    "    db = FAISS.load_local(\"faiss_index_pubmed\", embeddings)\n",
    "else:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "    docs = text_splitter.split_documents(data)  # 676307\n",
    "\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    db.save_local(\"faiss_index_pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset['train'][\"QUESTION\"][0]\n",
    "context = dataset['train'][\"CONTEXTS\"][0]\n",
    "\n",
    "retrieved_docs = db.similarity_search(question)  # db.similarity_search_with_score(question)\n",
    "\n",
    "print(f\"Question:\\n{question}\")\n",
    "print(f\"\\nContext:\\n{context}\")\n",
    "print(f\"\\nRetrieved document:\\n{retrieved_docs[0].page_content}\\n{retrieved_docs[1].page_content}\\n{retrieved_docs[2].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=PROMPT_TEMPLATE_QA_ANSWER_START,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "questions = dataset['train'][2:12][\"QUESTION\"]\n",
    "contexts = dataset['train'][2:12][\"CONTEXTS\"]\n",
    "long_answers = dataset['train'][2:12][\"LONG_ANSWER\"]\n",
    "final_decisions = dataset['train'][2:12][\"final_decision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA without Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_no_ret = []\n",
    "for question in questions:\n",
    "    # question = f'{question}. ALong with the answer, Explicitly state where the answer to the question is yes, no or maybe'\n",
    "    print(\"Question: \", question)\n",
    "    input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=False,\n",
    "            return_dict_in_generate=True,\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output.sequences[0][len(input_ids[0]):])\n",
    "    print(\"Generated Answer: \", output)\n",
    "\n",
    "    pred_no_ret.append(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_citations(predictions):\n",
    "    finalpred = []\n",
    "    for output in predictions:\n",
    "        output_with_citations = \"\"\n",
    "        citations = \"\"\n",
    "        citation_list = []\n",
    "\n",
    "        for lines in output.split(\"\\n\"):\n",
    "            lines = lines.strip()\n",
    "            if len(lines.split(\" \")) > 10:\n",
    "                for line in lines.split(\".\"):\n",
    "                    line = line.strip()\n",
    "                    docs_and_scores = db.similarity_search_with_score(line)[0]  # choosing top 1 relevant document\n",
    "                    if docs_and_scores[1] < 0.5:  # returned distance score is L2 distance, a lower score is better\n",
    "                        doc_content = docs_and_scores[0].page_content\n",
    "                        if doc_content in citation_list:\n",
    "                            idx = citation_list.index(doc_content)\n",
    "\n",
    "                        else:\n",
    "                            citation_list.append(doc_content)\n",
    "                            idx = len(citation_list)\n",
    "                            citations += f\"[{idx}] {doc_content}\\n\"\n",
    "\n",
    "                        output_with_citations += line + f\" [{idx}]. \"\n",
    "\n",
    "        final_output_with_citations = output_with_citations + \"\\n\\nCitations:\\n\" + citations\n",
    "        finalpred.append(final_output_with_citations)\n",
    "    return finalpred\n",
    "\n",
    "final_pred_citations = find_citations(pred_no_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_pred_citations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ret = []\n",
    "for question in questions:\n",
    "    print(\"Question: \", question)\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={'k': 3}\n",
    "    )\n",
    "\n",
    "    # retriever = db.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "    #                                  search_kwargs={\"score_threshold\": .5, \n",
    "    #                                                 \"k\": top_k})\n",
    "\n",
    "    rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain)\n",
    "\n",
    "    # QA with retrieval\n",
    "    qa_retrieval_result = rag_chain.invoke(question)\n",
    "    print(\"Generated Answer: \", qa_retrieval_result['text'])\n",
    "    pred_ret.append(qa_retrieval_result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\", cache_dir=\"evaluation_metrics\")  # value ranges from 0 to 1. score of 1 is better\n",
    "\n",
    "bleu_score = bleu.compute(predictions=pred_no_ret, references=long_answers)\n",
    "print(f\"Vanilla QA: BLEU Score: {bleu_score}\")\n",
    "\n",
    "bleu_score = bleu.compute(predictions=final_pred_citations, references=long_answers)\n",
    "print(f\"QA with Citations: BLEU Score: {bleu_score}\")\n",
    "\n",
    "bleu_score = bleu.compute(predictions=pred_ret, references=long_answers)\n",
    "print(f\"QA with Retrieval: BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\", cache_dir=\"evaluation_metrics\")\n",
    "\n",
    "bert_score = bertscore.compute(predictions=pred_no_ret, references=long_answers , lang=\"en\", batch_size =1)\n",
    "print(f\"Vanilla QA: BERTScore: {bert_score}\")\n",
    "\n",
    "bert_score = bertscore.compute(predictions=final_pred_citations, references=long_answers , lang=\"en\" , batch_size =1)\n",
    "print(f\"QA with Citations: BERTScore: {bert_score}\")\n",
    "\n",
    "bert_score = bertscore.compute(predictions=pred_ret, references=long_answers , lang=\"en\", batch_size =1 )\n",
    "print(f\"QA with Retrieval: BERTScore: {bert_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_calc_final(predictions, references):\n",
    "    acc = 0\n",
    "    for i in range(len(predictions)):\n",
    "        # print(references[i].lower(), predictions[i].lower())\n",
    "        if references[i].lower() in predictions[i].lower():\n",
    "            acc += 1\n",
    "    return acc / len(predictions)\n",
    "\n",
    "acc = acc_calc_final(predictions=pred_no_ret, references=final_decisions)\n",
    "print(f\"Vanilla QA: acc: {acc}\")\n",
    "\n",
    "acc = acc_calc_final(predictions=final_pred_citations, references=final_decisions)\n",
    "print(f\"QA with Citations: acc: {acc}\")\n",
    "\n",
    "acc = acc_calc_final(predictions=pred_ret, references=final_decisions)\n",
    "print(f\"QA with Retrieval: acc: {acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
