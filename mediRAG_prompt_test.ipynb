{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mediRAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from prompts import *\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)  # have to change depending on which device u wana use\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=False,\n",
    "# )\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"epfl-llm/meditron-7b\", model_kwargs={'cache_dir':\"models\", 'quantization_config':bnb_config}, device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt(question):\n",
    "#     prompt = f\"\"\"<|im_start|>system\n",
    "#     Answer the users questions:<|im_end|>\n",
    "#     <|im_start|>question\n",
    "#     {question}<|im_end|>\n",
    "#     <|im_start|>answer  \n",
    "#     \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "# Question=  'Is heart failure with preserved ejection fraction characterized by dynamic impairment of active relaxation and contraction of the left ventricle on exercise and associated with myocardial energy deficiency?'\n",
    "# pipe(get_prompt(Question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import HF_TOKEN\n",
    "model_name= 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "# model_name= 'epfl-llm/meditron-7b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,   cache_dir=\"models\") #, token = HF_TOKEN)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"models\",\n",
    "    device_map = \"auto\",# token = HF_TOKEN\n",
    "    # load_in_8bit = True,\n",
    "    # load_in_8bit_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bigbio/pubmed_qa\", cache_dir=\"data\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content_column = \"CONTEXTS\"\n",
    "\n",
    "def preprocess(dataset):\n",
    "    for split in dataset.keys():\n",
    "        for contexts in dataset[split][page_content_column]:\n",
    "            for sentence in contexts:\n",
    "                yield Document(page_content=sentence)\n",
    "\n",
    "data = list(preprocess(dataset))  # 655055\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device':'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model,   \n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs, \n",
    "    cache_folder=\"models\"\n",
    ")\n",
    "\n",
    "if os.path.exists(\"faiss_index_pubmed\"):\n",
    "    db = FAISS.load_local(\"faiss_index_pubmed\", embeddings)\n",
    "else:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "    docs = text_splitter.split_documents(data)  # 676307\n",
    "\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    db.save_local(\"faiss_index_pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset['train'][\"QUESTION\"][0]\n",
    "context = dataset['train'][\"CONTEXTS\"][0]\n",
    "long_answers = dataset['train'][\"LONG_ANSWER\"][0]\n",
    "final_decisions = dataset['train'][\"final_decision\"][0]\n",
    "\n",
    "retrieved_docs = db.similarity_search(question)  # db.similarity_search_with_score(question)\n",
    "\n",
    "print(f\"Question:\\n{question}\")\n",
    "print(f\"\\nContext:\\n{context}\")\n",
    "print(f\"Answer:\\n{long_answers}\")\n",
    "print(f\"\\nFinal_Decisions:\\n{final_decisions}\")\n",
    "print(f\"\\nRetrieved document:\\n{retrieved_docs[0].page_content}\\n{retrieved_docs[1].page_content}\\n{retrieved_docs[2].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=FEW_SHOT_PROMPT,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "questions = dataset['train'][2:102][\"QUESTION\"]\n",
    "contexts = dataset['train'][2:102][\"CONTEXTS\"]\n",
    "long_answers = dataset['train'][2:102][\"LONG_ANSWER\"]\n",
    "final_decisions = dataset['train'][2:102][\"final_decision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA without Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def get_prompt(question):\n",
    "    # prompt = \"\"\"Think Step by Step and Answer the users question:\n",
    "\n",
    "    # Question: {}  \n",
    "    # \"\"\".format(question)\n",
    "    prompt = f\"\"\"Answer the users question. An example is given below:\n",
    "\n",
    "    <example>\n",
    "    Question:\n",
    "    Does neurobehavioral disinhibition predict initiation of substance use in children with prenatal cocaine exposure?\n",
    "    Answer:\n",
    "    Yes, Prenatal drug exposure appears to be a risk pathway to ND, which by 8/9 years portends substance use initiation.\n",
    "    </example>\n",
    "    Question: {question}  \n",
    "    \"\"\"\n",
    "    return prompt    \n",
    "    # return f\"\"\"Answer the Users Questions with explainations\n",
    "    # Question: {question}\n",
    "    # Answer:\n",
    "    # \"\"\"\n",
    "\n",
    "# def get_prompt(question):\n",
    "#     prompt = f\"\"\"<|im_start|>system\n",
    "#     Answer the users questions:<|im_end|>\n",
    "#     <|im_start|>question\n",
    "#     {question}<|im_end|>\n",
    "#     <|im_start|>answer  \n",
    "#     \"\"\"\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "pred_no_ret = []\n",
    "for question in tqdm(questions):\n",
    "    # question = f'{question}. ALong with the answer, Explicitly state where the answer to the question is yes, no or maybe'\n",
    "    print(\"Question: \", question)\n",
    "    prompt = get_prompt(question)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "    # print(input_ids)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=False,\n",
    "            return_dict_in_generate=True,\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output.sequences[0][len(input_ids[0]):])\n",
    "    # output = text_generation_pipeline(prompt, CUDA_LAUNCH_BLOCKING=1)\n",
    "    # output = pipe(prompt)\n",
    "    print(\"Generated Answer: \", output)\n",
    "\n",
    "    pred_no_ret.append(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_citations(predictions):\n",
    "    finalpred = []\n",
    "    for output in predictions:\n",
    "        output_with_citations = \"\"\n",
    "        citations = \"\"\n",
    "        citation_list = []\n",
    "\n",
    "        for lines in output.split(\"\\n\"):\n",
    "            lines = lines.strip()\n",
    "            if len(lines.split(\" \")) > 10:\n",
    "                for line in lines.split(\".\"):\n",
    "                    line = line.strip()\n",
    "                    docs_and_scores = db.similarity_search_with_score(line)[0]  # choosing top 1 relevant document\n",
    "                    if docs_and_scores[1] < 0.5:  # returned distance score is L2 distance, a lower score is better\n",
    "                        doc_content = docs_and_scores[0].page_content\n",
    "                        if doc_content in citation_list:\n",
    "                            idx = citation_list.index(doc_content)\n",
    "\n",
    "                        else:\n",
    "                            citation_list.append(doc_content)\n",
    "                            idx = len(citation_list)\n",
    "                            citations += f\"[{idx}] {doc_content}\\n\"\n",
    "\n",
    "                        output_with_citations += line + f\" [{idx}]. \"\n",
    "\n",
    "        final_output_with_citations = output_with_citations + \"\\n\\nCitations:\\n\" + citations\n",
    "        finalpred.append(final_output_with_citations)\n",
    "    return finalpred\n",
    "\n",
    "final_pred_citations = find_citations(pred_no_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_pred_citations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "pred_ret = []\n",
    "for question in tqdm(questions):\n",
    "    print(\"Question: \", question)\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={'k': 3}\n",
    "    )\n",
    "\n",
    "    # retriever = db.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "    #                                  search_kwargs={\"score_threshold\": .5, \n",
    "    #                                                 \"k\": top_k})\n",
    "\n",
    "    rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain)\n",
    "\n",
    "    # QA with retrieval\n",
    "    qa_retrieval_result = rag_chain.invoke(question)\n",
    "    print(\"Generated Answer: \", qa_retrieval_result['text'])\n",
    "    pred_ret.append(qa_retrieval_result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\"model\": model_name,\n",
    "        \"prompt\": FEW_SHOT_PROMPT,\n",
    "        # \"TOT\": {'results':pred_ret},\n",
    "        \"Vanilla\": {'results': pred_no_ret}, #, 'metrics': {'BertScore': bert_score_no_ret, 'BLEU': bleu_score_no_ret, \"Accuracy\": acc_no_ret, \"Perplexity\":perp_no_ret}},\n",
    "         \"With Citations\": { 'results':final_pred_citations}, #, 'metrics': {'BertScore': bert_score_citations, 'BLEU': bleu_score_citations, \"Accuracy\": acc_citations, \"Perplexity\":perp_citations}},\n",
    "           \"With Retrieval\": {'results':pred_ret}, #, 'metrics': {'BertScore': bert_score_ret, 'BLEU': bleu_score_ret, \"Accuracy\": acc_ret, \"Perplexity\":perp_ret}}, \n",
    "           \"ground_truth\": {\"long_answers\": long_answers, \"final_decisions\": final_decisions}}\n",
    "\n",
    "json_data = json.dumps(data, indent=2)\n",
    "\n",
    "with open('oneshot_Full_results.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate\n",
    "\n",
    "# Read the JSON data from the file\n",
    "with open('oneshot_Full_results.json', 'r') as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "\n",
    "# Access the lists from the loaded data\n",
    "pred_no_ret = loaded_data['Vanilla']['results']\n",
    "final_pred_citations = loaded_data['With Citations']['results']\n",
    "pred_ret = loaded_data['With Retrieval']['results']\n",
    "long_answers = loaded_data['ground_truth']['long_answers']\n",
    "final_decisions = loaded_data['ground_truth']['final_decisions']\n",
    "\n",
    "# def preprocess(preds):\n",
    "#     new_pred = []\n",
    "#     for pred in preds:\n",
    "#         new_pred.append(pred[5:])\n",
    "#     return new_pred\n",
    "\n",
    "def clean_cit(preds):\n",
    "    new_pred = []\n",
    "    for pred in preds:\n",
    "        new_pred.append(pred.split(\"\\n\\nCitations:\\n\")[0])\n",
    "    return new_pred\n",
    "\n",
    "def rem_comment(preds):\n",
    "    new_pred = []\n",
    "    for pred in preds:\n",
    "        new_pred.append(pred.split(\"\\n\\nComment\")[0])\n",
    "    return new_pred\n",
    "\n",
    "# pred_no_ret = preprocess(pred_no_ret)\n",
    "final_pred_citations = clean_cit(final_pred_citations)\n",
    "# pred_ret = rem_comment(pred_ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla QA: BLEU Score: {'bleu': 0.053022689170644156, 'precisions': [0.344866491162091, 0.12270418132082844, 0.06588043920292802, 0.03603221704111912], 'brevity_penalty': 0.5296296529571533, 'length_ratio': 0.6114049206714187, 'translation_length': 2659, 'reference_length': 4349}\n",
      "QA with Citations: BLEU Score: {'bleu': 0.04331043272050909, 'precisions': [0.364613476643241, 0.11929371231696813, 0.06157303370786517, 0.03195488721804511], 'brevity_penalty': 0.45029590241911394, 'length_ratio': 0.5562198206484249, 'translation_length': 2419, 'reference_length': 4349}\n",
      "QA with Retrieval: BLEU Score: {'bleu': 0.02859670911874478, 'precisions': [0.12182410423452769, 0.03769595333576376, 0.01696658097686375, 0.008583055863854976], 'brevity_penalty': 1.0, 'length_ratio': 3.17659232007358, 'translation_length': 13815, 'reference_length': 4349}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\", cache_dir=\"evaluation_metrics\")  # value ranges from 0 to 1. score of 1 is better\n",
    "\n",
    "bleu_score_no_ret = bleu.compute(predictions=pred_no_ret, references=long_answers)\n",
    "print(f\"Vanilla QA: BLEU Score: {bleu_score_no_ret}\")\n",
    "\n",
    "bleu_score_citations = bleu.compute(predictions=final_pred_citations, references=long_answers)\n",
    "print(f\"QA with Citations: BLEU Score: {bleu_score_citations}\")\n",
    "\n",
    "bleu_score_ret = bleu.compute(predictions=pred_ret, references=long_answers)\n",
    "print(f\"QA with Retrieval: BLEU Score: {bleu_score_ret}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla QA: BERTScore: {'precision': 0.8677240765094757, 'recall': 0.8686194777488708, 'f1': 0.8679354679584503, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.35.2)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA with Citations: BERTScore: {'precision': 0.8474992269277573, 'recall': 0.8440714913606644, 'f1': 0.845581641793251, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.35.2)'}\n",
      "QA with Retrieval: BERTScore: {'precision': 0.8225240308046341, 'recall': 0.8677024567127227, 'f1': 0.8440739339590073, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.35.2)'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\", cache_dir=\"evaluation_metrics\")\n",
    "\n",
    "bert_score_no_ret = bertscore.compute(predictions=pred_no_ret, references=long_answers , lang=\"en\", batch_size =1)\n",
    "bert_score_no_ret = {key: np.mean(value) if key!= \"hashcode\" else value for key, value in bert_score_no_ret.items()}\n",
    "print(f\"Vanilla QA: BERTScore: {bert_score_no_ret}\")\n",
    "\n",
    "bert_score_citations = bertscore.compute(predictions=final_pred_citations, references=long_answers , lang=\"en\" , batch_size =1)\n",
    "bert_score_citations = {key: np.mean(value) if key!= \"hashcode\" else value for key, value in bert_score_citations.items()}\n",
    "print(f\"QA with Citations: BERTScore: {bert_score_citations}\")\n",
    "\n",
    "bert_score_ret = bertscore.compute(predictions=pred_ret, references=long_answers , lang=\"en\", batch_size =1 )\n",
    "bert_score_ret = {key: np.mean(value) if key!= \"hashcode\" else value for key, value in bert_score_ret.items()}\n",
    "print(f\"QA with Retrieval: BERTScore: {bert_score_ret}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f0a4f5119444fbbd77cdc0e1154875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.20494256973267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3dee63491c4b3ba48d7419518c325c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.3957336640358\n"
     ]
    }
   ],
   "source": [
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\", cache_dir=\"evaluation_metrics\")\n",
    "\n",
    "perp_no_ret = perplexity.compute(model_id='gpt2',\n",
    "                             add_start_token=False,\n",
    "                             predictions=pred_no_ret, \n",
    "                             batch_size =2)\n",
    "print(perp_no_ret['mean_perplexity'])\n",
    "\n",
    "# perp_citations = perplexity.compute(model_id='gpt2',\n",
    "#                              add_start_token=False,\n",
    "#                              predictions=final_pred_citations, \n",
    "#                              batch_size =2)\n",
    "# print(perp_citations['mean_perplexity'])\n",
    "\n",
    "perp_ret = perplexity.compute(model_id='gpt2',\n",
    "                             add_start_token=False,\n",
    "                             predictions=pred_ret, \n",
    "                             batch_size =2)\n",
    "print(perp_ret['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ptejd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ptejd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ptejd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': 0.1715961297070393}\n",
      "{'meteor': 0.19560881179840028}\n",
      "{'meteor': 0.23214835260102568}\n"
     ]
    }
   ],
   "source": [
    "meteor = evaluate.load('meteor', cache_dir=\"evaluation_metrics\")\n",
    "met_no_ret = meteor.compute(predictions=pred_no_ret, references=long_answers)\n",
    "print(met_no_ret)\n",
    "met_citations = meteor.compute(predictions=final_pred_citations, references=long_answers)\n",
    "print(met_citations)\n",
    "met_ret = meteor.compute(predictions=pred_ret, references=long_answers)\n",
    "print(met_ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla QA: acc: 0.69\n",
      "QA with Citations: acc: 0.66\n",
      "QA with Retrieval: acc: 0.85\n"
     ]
    }
   ],
   "source": [
    "def acc_calc_final(predictions, references):\n",
    "    acc = 0\n",
    "    for i in range(len(predictions)):\n",
    "        # print(references[i].lower(), predictions[i].lower())\n",
    "        if references[i].lower() in predictions[i][:15].lower():\n",
    "            acc += 1\n",
    "    return acc / len(predictions)\n",
    "\n",
    "acc_no_ret = acc_calc_final(predictions=pred_no_ret, references=final_decisions)\n",
    "print(f\"Vanilla QA: acc: {acc_no_ret}\")\n",
    "\n",
    "acc_citations = acc_calc_final(predictions=final_pred_citations, references=final_decisions)\n",
    "print(f\"QA with Citations: acc: {acc_citations}\")\n",
    "\n",
    "acc_ret = acc_calc_final(predictions=pred_ret, references=final_decisions)\n",
    "print(f\"QA with Retrieval: acc: {acc_ret}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\"model\": loaded_data['model'],\n",
    "        \"prompt\": loaded_data['prompt'],\n",
    "        # \"COT\": {'results':pred_ret}, 'metrics': {'BertScore': bert_score_ret, 'BLEU': bleu_score_ret, \"Accuracy\": acc_ret}, #, \"Perplexity\":perp_ret}},\n",
    "        \"Vanilla\": {'results': pred_no_ret, 'metrics': {'BertScore': bert_score_no_ret, 'BLEU': bleu_score_no_ret, \"Accuracy\": acc_no_ret,  \"Perplexity\":perp_no_ret['mean_perplexity'], \"Meteor\": met_no_ret['meteor']}},  #\n",
    "         \"With Citations\": { 'results':final_pred_citations, 'metrics': {'BertScore': bert_score_citations, 'BLEU': bleu_score_citations, \"Accuracy\": acc_citations, \"Meteor\": met_citations['meteor']}},  # \"Perplexity\":perp_citations\n",
    "           \"With Retrieval\": {'results':pred_ret, 'metrics': {'BertScore': bert_score_ret, 'BLEU': bleu_score_ret, \"Accuracy\": acc_ret, \"Perplexity\":perp_ret['mean_perplexity'], \"Meteor\": met_ret['meteor']}},  # \n",
    "           \"ground_truth\": {\"long_answers\": long_answers, \"final_decisions\": final_decisions}}\n",
    "\n",
    "json_data = json.dumps(data, indent=2)\n",
    "\n",
    "with open('oneshot_Full_results_metrics_cleanret.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
