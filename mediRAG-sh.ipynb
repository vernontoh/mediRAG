{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mediRAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm as tqdm\n",
    "import numpy as np\n",
    "# import pickle\n",
    "from uuid import uuid4\n",
    "\n",
    "from prompts import *\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline #BitsAndBytesConfig\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.set_device(3)  # have to change depending on which device u wana use\n",
    "# torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"models\")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=False,\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     cache_dir=\"models\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Util functions\n",
    "# q2c = {}\n",
    "# def question2chunk(dataset):\n",
    "#     question = \"QUESTION\"\n",
    "#     context = \"CONTEXTS\"\n",
    "#     for split in dataset.keys():\n",
    "#         for abstract in dataset[split][context]:\n",
    "#             for sentence in abstract:\n",
    "#                 yield Document(page_content=sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
       "        num_rows: 11269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"bigbio/pubmed_qa\", cache_dir=\"data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='In previous work we (Fisher et al., 2011) examined the emergence of neurobehavioral disinhibition (ND) in adolescents with prenatal substance exposure. We computed ND factor scores at three age points (8/9, 11 and 13/14 years) and found that both prenatal substance exposure and early adversity predicted ND. The purpose of the current study was to determine the association between these ND scores and initiation of substance use between ages 8 and 16 in this cohort as early initiation of substance use has been related to later substance use disorders. Our hypothesis was that prenatal cocaine exposure predisposes the child to ND, which, in turn, is associated with initiation of substance use by age 16.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(dataset):\n",
    "    page_content_column = \"CONTEXTS\"\n",
    "    for split in dataset.keys():\n",
    "        for contexts in dataset[split][page_content_column]:\n",
    "            for sentence in contexts:\n",
    "                yield Document(page_content=sentence)\n",
    "\n",
    "data = list(preprocess(dataset))  # 655055\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device':'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model,   \n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs, \n",
    "    cache_folder=\"models\"\n",
    ")\n",
    "\n",
    "if os.path.exists(\"faiss_index_pubmed\"):\n",
    "    print(os.path.exists(\"faiss_index_pubmed\"))\n",
    "    db = FAISS.load_local(\"faiss_index_pubmed\", embeddings)\n",
    "else:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "    docs = text_splitter.split_documents(data)  # 676307\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    db.save_local(\"faiss_index_pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(data)\n",
    "# bm25_retriever.k = 3\n",
    "\n",
    "faiss_retriever = db.as_retriever() #search_kwargs={\"k\": 3}\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Does neurobehavioral disinhibition predict initiation of substance use in children with prenatal cocaine exposure?\n",
      "\n",
      "Context:\n",
      "['In previous work we (Fisher et al., 2011) examined the emergence of neurobehavioral disinhibition (ND) in adolescents with prenatal substance exposure. We computed ND factor scores at three age points (8/9, 11 and 13/14 years) and found that both prenatal substance exposure and early adversity predicted ND. The purpose of the current study was to determine the association between these ND scores and initiation of substance use between ages 8 and 16 in this cohort as early initiation of substance use has been related to later substance use disorders. Our hypothesis was that prenatal cocaine exposure predisposes the child to ND, which, in turn, is associated with initiation of substance use by age 16.', \"We studied 386 cocaine exposed and 517 unexposed children followed since birth in a longitudinal study. Five dichotomous variables were computed based on the subject's report of substance use: alcohol only; tobacco only; marijuana only; illicit substances and any substance.\", 'Cox proportional hazard regression showed that the 8/9 year ND score was related to initiation of alcohol, tobacco, illicit and any substance use but not marijuana use. The trajectory of ND across the three age periods was related to substance use initiation in all five substance use categories. Prenatal cocaine exposure, although initially related to tobacco, marijuana and illicit substance initiation, was no longer significant with ND scores in the models.']\n",
      "\n",
      "Retrieved document:\n",
      "In previous work we (Fisher et al., 2011) examined the emergence of neurobehavioral disinhibition (ND) in adolescents with prenatal substance exposure. We computed ND factor scores at three age points (8/9, 11 and 13/14 years) and found that both prenatal substance exposure and early adversity predicted ND. The purpose of the current study was to determine the association between these ND scores and initiation of substance use between ages 8 and 16 in this cohort as early initiation of substance use has been related to later substance use disorders. Our hypothesis was that prenatal cocaine exposure predisposes the child to ND, which, in turn, is associated with initiation of substance use by age 16.\n",
      "This longitudinal study had three aims: 1) determine the extent to which boys at high average risk and low average risk for substance use disorder differ on a construct of neurobehavioral disinhibition, 2) evaluate the capacity of neurobehavioral disinhibition to predict substance use frequency at age 16, and 3) demonstrate the utility of neurobehavioral disinhibition in predicting substance use disorder.\n",
      "The authors derived an index of neurobehavioral disinhibition from measures of affect, behavior, and cognition. The neurobehavioral disinhibition score was used to discriminate youth at high and low average risk for substance use disorder and to predict substance use frequency after 4-6 years and substance use disorder after 7-9 years.\n"
     ]
    }
   ],
   "source": [
    "question = dataset['train'][\"QUESTION\"][0]\n",
    "context = dataset['train'][\"CONTEXTS\"][0]\n",
    "\n",
    "retrieved_docs = ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "print(f\"Question:\\n{question}\")\n",
    "print(f\"\\nContext:\\n{context}\")\n",
    "print(f\"\\nRetrieved document:\\n{retrieved_docs[0].page_content}\\n{retrieved_docs[1].page_content}\\n{retrieved_docs[2].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:02<00:00, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PX implementation\n",
    "\n",
    "def calculate_map(ranked_lists):\n",
    "    ap_sum = 0\n",
    "    for ranked_list in ranked_lists:\n",
    "        precision_sum = 0\n",
    "        relevant_docs = 0\n",
    "        for i, doc in enumerate(ranked_list):\n",
    "            if doc == 1:\n",
    "                relevant_docs += 1\n",
    "                precision_sum += relevant_docs / (i + 1)\n",
    "        if relevant_docs != 0:\n",
    "            ap_sum += precision_sum / relevant_docs\n",
    "        else:\n",
    "            ap_sum += 0\n",
    "    map = ap_sum / len(ranked_lists)\n",
    "    return map\n",
    "\n",
    "def preprocess_val(val_dataset):\n",
    "    page_content_column = \"CONTEXTS\"\n",
    "    for contexts in val_dataset[page_content_column]:\n",
    "        for sentence in contexts:\n",
    "            yield Document(page_content=sentence)\n",
    "\n",
    "from collections import defaultdict\n",
    "val_dataset = dataset['validation']\n",
    "val_data = list(preprocess_val(val_dataset)) # complete docs\n",
    "\n",
    "# shorten\n",
    "val_data = val_data[0:20]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "val_chunks = text_splitter.split_documents(val_data)\n",
    "\n",
    "# check which docs have been split into chunks\n",
    "docs_split_into_chunks = []\n",
    "for idx, complete_doc in enumerate(val_data):\n",
    "    split_into_chunks = True\n",
    "    for possibly_incomplete_doc in val_chunks:\n",
    "        if complete_doc == possibly_incomplete_doc:\n",
    "            split_into_chunks = False\n",
    "    if split_into_chunks == True:\n",
    "        docs_split_into_chunks.append(idx)\n",
    "\n",
    "# for those docs split into chunks, check number of corresponding chunks\n",
    "chunks_dict = defaultdict(int)\n",
    "for doc_idx in docs_split_into_chunks:\n",
    "    full_doc_content = val_data[doc_idx].page_content\n",
    "    for chunk in val_chunks:\n",
    "        if chunk.page_content in full_doc_content:\n",
    "            chunks_dict[doc_idx] += 1\n",
    "\n",
    "ranked_lists = []\n",
    "for i in tqdm(range(len(val_data))): # for each validation query\n",
    "    question = dataset['validation'][\"QUESTION\"][i]\n",
    "    context = dataset['validation'][\"CONTEXTS\"][i]\n",
    "    joined_context = ''.join(context)\n",
    "    # retrieved_docs = db.similarity_search(question)\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(question)\n",
    "    \n",
    "    ranked_ls = []\n",
    "    if i not in docs_split_into_chunks:\n",
    "        num_chunks = 1\n",
    "    else:\n",
    "        num_chunks = chunks_dict[i]\n",
    "    for j in range(num_chunks): # num of elements within a nested list\n",
    "        retrieved_doc_content = retrieved_docs[j].page_content # change this -> retrieved_docs[0]: 1st ranked doc, retrieved_docs[1]: 2nd ranked doc, and so on\n",
    "        if retrieved_doc_content in joined_context:\n",
    "            ranked_ls.append(1)\n",
    "        else:\n",
    "            ranked_ls.append(0)\n",
    "    ranked_lists.append(ranked_ls)\n",
    "\n",
    "print(calculate_map(ranked_lists))\n",
    "print(ranked_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Processing data\n",
      "Creating dict mapping question to chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:14<04:40, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:30<04:32, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:45<04:19, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:00<04:02, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:15<03:48, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:30<03:32, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:47<03:24, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [02:02<03:02, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [02:16<02:45, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:31<02:29, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:45<02:12, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [02:59<01:56, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [03:15<01:43, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [03:31<01:31, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [03:46<01:15, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [04:00<00:59, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [04:16<00:45, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [04:31<00:30, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [04:46<00:14, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "Retrieving documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:00<00:00, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label relevance\n",
      "-------------\n",
      "[[1, 1, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [0, 1, 0, 1, 1, 0, 0, 0], [1, 0, 1, 0, 1, 0, 0], [1, 1, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0], [1, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 1], [1, 1, 0, 0, 1, 0, 0], [1, 1, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1], [1, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SH implementation\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def preprocess_val(val_contexts):\n",
    "    page_content_column = \"CONTEXTS\"\n",
    "    for contexts in val_contexts:\n",
    "        for sentence in contexts:\n",
    "            yield Document(page_content=sentence)\n",
    "\n",
    "def create_question2chunk(val_questions, val_contexts):\n",
    "    question2chunk = {}\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "    for question, context in zip(val_questions, val_contexts):\n",
    "        # split each doc into different sentences then chunk them\n",
    "        context_docs = []\n",
    "        for sentence in context:\n",
    "            doc = Document(page_content=sentence)\n",
    "            context_docs.append(doc)\n",
    "        chunks = text_splitter.split_documents(context_docs)\n",
    "        question2chunk[question] = chunks\n",
    "    return question2chunk\n",
    "\n",
    "print(\"Loading data\")\n",
    "val_dataset = dataset['validation']\n",
    "val_contexts = val_dataset[\"CONTEXTS\"][:20]\n",
    "val_questions = val_dataset[\"QUESTION\"][:20]\n",
    "\n",
    "print(\"Processing data\")\n",
    "val_data = list(preprocess_val(val_contexts)) # complete docs\n",
    "\n",
    "print(\"Creating dict mapping question to chunks\")\n",
    "question2chunk = create_question2chunk(val_questions,val_contexts)\n",
    "\n",
    "all_relevance = []\n",
    "for i in tqdm(range(len(val_questions))): # for each validation query\n",
    "    print(\"Retrieving documents\")\n",
    "    question = val_questions[i]\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "    print(\"Label relevance\")\n",
    "    # label the retrieved list of doc as relevant or not\n",
    "    bin_relevance = []\n",
    "    for d in retrieved_docs:\n",
    "        gt_chunks = question2chunk[question]\n",
    "        if d in gt_chunks:\n",
    "            bin_relevance.append(1)\n",
    "        else:\n",
    "            bin_relevance.append(0)\n",
    "    all_relevance.append(bin_relevance)\n",
    "    print(\"-------------\")\n",
    "\n",
    "print(all_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k (This we solve for you!)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "    Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in\n",
    "    enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    #write your code here\n",
    "    # print(r)\n",
    "    relevant_idx = [i+1 for i in list(np.where(np.array(r)==1)[0])]\n",
    "    # print(relevant_idx)\n",
    "    n = sum(r)\n",
    "    if n == 0: \n",
    "        return 0\n",
    "    else:\n",
    "        precision_k = [precision_at_k(r,pk) for pk in relevant_idx]\n",
    "        avg_p = 1/n * sum(precision_k)\n",
    "        return avg_p\n",
    "    \n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    #write your code here\n",
    "    avg_precision = [average_precision(r) for r in rs]\n",
    "    n = len(rs)\n",
    "    m_avg_p = 1/n * sum(avg_precision)\n",
    "\n",
    "    return m_avg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8840277777777777\n"
     ]
    }
   ],
   "source": [
    "score = mean_average_precision(all_relevance)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# def preprocess_val(val_contexts):\n",
    "#     page_content_column = \"CONTEXTS\"\n",
    "#     for contexts in val_contexts:\n",
    "#         for sentence in contexts:\n",
    "#             yield Document(page_content=sentence)\n",
    "\n",
    "# def create_question2chunk(val_questions, val_contexts):\n",
    "#     question2chunk = {}\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "#     for question, context in zip(val_questions, val_contexts):\n",
    "#         # split each doc into different sentences then chunk them\n",
    "#         context_docs = []\n",
    "#         for sentence in context:\n",
    "#             doc = Document(page_content=sentence)\n",
    "#             context_docs.append(doc)\n",
    "#         chunks = text_splitter.split_documents(context_docs)\n",
    "#         question2chunk[question] = chunks\n",
    "#     return question2chunk\n",
    "\n",
    "# val_dataset = dataset['validation']\n",
    "# val_contexts = val_dataset[\"CONTEXTS\"][:1000]\n",
    "# val_questions = val_dataset[\"QUESTION\"][:1000]\n",
    "\n",
    "# val_data = list(preprocess_val(val_contexts)) # complete docs\n",
    "# out = create_question2chunk(val_questions,val_contexts)\n",
    "\n",
    "# print(len(out))\n",
    "# o = out['Do posterior fossa and spinal gangliogliomas form two distinct clinicopathologic and molecular subgroups?']\n",
    "# Document(page_content='Gangliogliomas are low-grade glioneuronal tumors of the central nervous system and the commonest cause of chronic intractable epilepsy. Most gangliogliomas (>70%) arise in the temporal lobe, and infratentorial tumors account for less than 10%. Posterior fossa gangliogliomas can have the features of a classic supratentorial tumor or a pilocytic astrocytoma with focal gangliocytic differentiation, and this observation led to the hypothesis tested in this study - gangliogliomas of the posterior fossa and spinal cord consist of two morphologic types that can be distinguished by specific genetic alterations.') in o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=PROMPT_TEMPLATE_QA_EXPLAINER,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "question = dataset['train'][2][\"QUESTION\"]\n",
    "context = dataset['train'][2][\"CONTEXTS\"]\n",
    "long_answer = dataset['train'][2][\"LONG_ANSWER\"]\n",
    "final_decision = dataset['train'][2][\"final_decision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA without Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n## Abstract\\n\\nHeart failure with preserved ejection fraction (HFpEF) is a common form of heart failure characterized by impaired left ventricular (LV) diastolic function and normal or near-normal LV ejection fraction. The pathophysiology of HFpEF is complex and involves both structural and functional changes in the heart. Recent studies have shown that HFpEF is associated with dynamic impairment of active relaxation and contraction of the LV on exercise, which may be related to myocardial energy deficiency. This review summarizes the current evidence on the dynamic impairment of LV function in HFpEF and its association with myocardial energy deficiency.\\n\\n## Introduction\\n\\nHeart failure with preserved ejection fraction (HFpEF) is a common form of heart failure characterized by impaired left ventricular (LV) diastolic function and normal or near-normal LV ejection fraction. HFpEF is estimated to affect 50% of all patients with heart failure, and its prevalence is increasing with age. The pathophysiology of HFpEF is complex and involves both structural and functional changes in the heart. Recent studies have shown that HFpEF is associated with dynamic impairment of active relaxation and contraction of the LV on exercise, which may be related to myocardial'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=300,\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output.sequences[0][len(input_ids[0]):])\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_with_citations = \"\"\n",
    "citations = \"\"\n",
    "citation_list = []\n",
    "\n",
    "for lines in output.split(\"\\n\"):\n",
    "    lines = lines.strip()\n",
    "    if len(lines.split(\" \")) > 10:\n",
    "        for line in lines.split(\".\"):\n",
    "            line = line.strip()\n",
    "            docs_and_scores = db.similarity_search_with_score(line)[0]  # choosing top 1 relevant document\n",
    "            if docs_and_scores[1] < 0.5:  # returned distance score is L2 distance, a lower score is better\n",
    "                doc_content = docs_and_scores[0].page_content\n",
    "                if doc_content in citation_list:\n",
    "                    idx = citation_list.index(doc_content)\n",
    "\n",
    "                else:\n",
    "                    citation_list.append(doc_content)\n",
    "                    idx = len(citation_list)\n",
    "                    citations += f\"[{idx}] {doc_content}\\n\"\n",
    "\n",
    "                output_with_citations += line + f\" [{idx}]. \"\n",
    "\n",
    "final_output_with_citations = output_with_citations + \"\\n\\nCitations:\\n\" + citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart failure with preserved ejection fraction (HFpEF) is a common form of heart failure characterized by impaired left ventricular (LV) diastolic function and normal or near-normal LV ejection fraction [1]. The pathophysiology of HFpEF is complex and involves both structural and functional changes in the heart [2]. Recent studies have shown that HFpEF is associated with dynamic impairment of active relaxation and contraction of the LV on exercise, which may be related to myocardial energy deficiency [3]. This review summarizes the current evidence on the dynamic impairment of LV function in HFpEF and its association with myocardial energy deficiency [1]. Heart failure with preserved ejection fraction (HFpEF) is a common form of heart failure characterized by impaired left ventricular (LV) diastolic function and normal or near-normal LV ejection fraction [0]. HFpEF is estimated to affect 50% of all patients with heart failure, and its prevalence is increasing with age [4]. The pathophysiology of HFpEF is complex and involves both structural and functional changes in the heart [1]. Recent studies have shown that HFpEF is associated with dynamic impairment of active relaxation and contraction of the LV on exercise, which may be related to myocardial [2]. \n",
      "\n",
      "Citations:\n",
      "[1] Nearly half of patients with heart failure have a preserved ejection fraction (HFpEF). Symptoms of exercise intolerance and dyspnea are most often attributed to diastolic dysfunction; however, impaired systolic and/or arterial vasodilator reserve under stress could also play an important role.\n",
      "[2] Patients with HFpEF (n=17) and control subjects without heart failure (n=19) generally matched for age, gender, hypertension, diabetes mellitus, obesity, and the presence of left ventricular hypertrophy underwent maximal-effort upright cycle ergometry with radionuclide ventriculography to determine rest and exercise cardiovascular function. Resting cardiovascular function was similar between the 2 groups. Both had limited exercise capacity, but this was more profoundly reduced in HFpEF patients (exercise duration 180+/-71 versus 455+/-184 seconds; peak oxygen consumption 9.0+/-3.4 versus 14.4+/-3.4 mL x kg(-1) x min(-1); both P<0.001). At matched low-level workload, HFpEF subjects displayed approximately 40% less of an increase in heart rate and cardiac output and less systemic vasodilation (all P<0.05) despite a similar rise in end-diastolic volume, stroke volume, and contractility. Heart rate recovery after exercise was also significantly delayed in HFpEF patients. Exercise capacity correlated with the\n",
      "[3] We sought to evaluate the role of exercise-related changes in left ventricular (LV) relaxation and of LV contractile function and vasculoventricular coupling (VVC) in the pathophysiology of heart failure with preserved ejection fraction (HFpEF) and to assess myocardial energetic status in these patients.\n",
      "[4] Forty-one HFpEF patients having an echocardiographic left ventricular ejection fraction ≥ 40% and 12 age-matched volunteers without heart failure underwent the echocardiographic examination and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(final_output_with_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='We sought to evaluate the role of exercise-related changes in left ventricular (LV) relaxation and of LV contractile function and vasculoventricular coupling (VVC) in the pathophysiology of heart failure with preserved ejection fraction (HFpEF) and to assess myocardial energetic status in these patients.'),\n",
       "  Document(page_content='Nearly half of patients with heart failure have a preserved ejection fraction (HFpEF). Symptoms of exercise intolerance and dyspnea are most often attributed to diastolic dysfunction; however, impaired systolic and/or arterial vasodilator reserve under stress could also play an important role.'),\n",
       "  Document(page_content='To investigate the associations between glucose metabolism, left ventricular (LV) contractile reserve, and exercise capacity in patients with chronic systolic heart failure (HF).')],\n",
       " 'question': 'Is heart failure with preserved ejection fraction characterized by dynamic impairment of active relaxation and contraction of the left ventricle on exercise and associated with myocardial energy deficiency?',\n",
       " 'text': ' Heart failure with preserved ejection fraction (HFpEF) is a type of heart failure that is characterized by a stiffening of the left ventricle (LV) and impaired diastolic function. However, recent studies have suggested that impaired systolic and/or arterial vasodilator reserve under stress could also play an important role in the pathophysiology of HFpEF.\\n\\nIn terms of exercise capacity, patients with HFpEF often experience symptoms of exercise intolerance and dyspnea, which are most often attributed to diastolic dysfunction. However, recent studies have suggested that dynamic impairment of active relaxation and contraction of the LV on exercise could also contribute to exercise intolerance in patients with HFpEF.\\n\\nAdditionally, myocardial energy deficiency has been associated with HFpEF. This could be due to a variety of factors, including reduced cardiac output, increased oxygen demand, and impaired mitochondrial function.\\n\\nOverall, while HFpEF is characterized by dynamic impairment of active relaxation and contraction of the LV on exercise, myocardial energy deficiency is also a common finding in these patients. Further research is needed to fully understand the role of these factors in the pathophysiology of HFpEF and to develop effective treatments for this condition.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 3}\n",
    ")\n",
    "\n",
    "# retriever = db.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "#                                  search_kwargs={\"score_threshold\": .5, \n",
    "#                                                 \"k\": top_k})\n",
    "\n",
    "rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain)\n",
    "\n",
    "# QA with retrieval\n",
    "qa_retrieval_result = rag_chain.invoke(question)\n",
    "\n",
    "qa_retrieval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.0, 'precisions': [0.05627705627705628, 0.013043478260869565, 0.004366812227074236, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 9.625, 'translation_length': 231, 'reference_length': 24}\n",
      "BLEU Score: {'bleu': 0.0, 'precisions': [0.07075471698113207, 0.018957345971563982, 0.004761904761904762, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 8.833333333333334, 'translation_length': 212, 'reference_length': 24}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\", cache_dir=\"evaluation_metrics\")  # value ranges from 0 to 1. score of 1 is better\n",
    "\n",
    "bleu_score = bleu.compute(predictions=[output_with_citations], references=[long_answer])\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "bleu_score = bleu.compute(predictions=[qa_retrieval_result[\"text\"]], references=[long_answer])\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': [0.7966367602348328], 'recall': [0.8586264848709106], 'f1': [0.8264709115028381], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.35.1)'}\n",
      "BERTScore: {'precision': [0.8198319673538208], 'recall': [0.8714783191680908], 'f1': [0.8448666334152222], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.35.1)'}\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\", cache_dir=\"evaluation_metrics\")\n",
    "\n",
    "bert_score = bertscore.compute(predictions=[output_with_citations], references=[long_answer], lang=\"en\")\n",
    "print(f\"BERTScore: {bert_score}\")\n",
    "\n",
    "bert_score = bertscore.compute(predictions=[qa_retrieval_result[\"text\"]], references=[long_answer], lang=\"en\")\n",
    "print(f\"BERTScore: {bert_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
